ğŸ§  NLP with Transformers â€“ Code Implementations
This repository contains my personal code implementations and experiments based on the book Natural Language Processing with Transformers by Lewis Tunstall, Leandro von Werra, and Thomas Wolf (published by Oâ€™Reilly Media and Hugging Face).

ğŸ“š About the Book
The book provides a hands-on, project-driven approach to Natural Language Processing (NLP) using state-of-the-art Transformer models like BERT, RoBERTa, GPT, T5, and more. It covers:

- Preprocessing text for Transformers

- Fine-tuning models for classification, summarization, and question answering

- Building pipelines and production-ready tools

- Interpretability and fairness in NLP

- Deployment with the Hugging Face ğŸ¤— ecosystem

âš™ï¸ Implementation Notes
Due to limited computational resources, I adapted some implementations by:

- Using smaller or distilled versions of models where possible

- Leveraging techniques like QLoRA for parameter-efficient fine-tuning

- Optimizing batch sizes and using CPU-compatible alternatives when needed

These changes helped maintain a balance between accessibility and the core learning objectives of each project.

