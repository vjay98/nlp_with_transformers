{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9389d6d",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "## Greedy Search Decoding\n",
    "\n",
    "The simplest decoding method to get discrete tokens from a model's continuous output is to greedily select the token with the highest probability at each timestep:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t = \\arg\\max P(y_t \\mid y_{<t}, x)\n",
    "$$\n",
    "\n",
    "Lets see how greedy search works by loading 1.5 billion -parameter version of GPT-2 with a language modeling head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f643cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799f1d9c0aff4dbe8c92aa98a78964f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcc54606098412ba9a7d326fac401f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583d690d1e5b46f6b928ccf6c7d4ac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad16d6e352714a2d82253910ba947503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda3be6d6c424592bc6eb1128cbe4853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 11:21:22.490930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747979482.505822  170848 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747979482.510496  170848 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747979482.523244  170848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747979482.523270  170848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747979482.523272  170848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747979482.523274  170848 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-23 11:21:22.529957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7f2b0094bf4f1994b021f642fb5481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82210d74f3544059bd930580f0ca8563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15830f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"transformers are the \"\n",
    "input_ids = tokenizer(input_txt, return_tensors = 'pt')['input_ids'].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b84e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        ## Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        ## Store tokens with highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4de70d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Choice 1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Choice 2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Choice 3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Choice 4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Choice 5",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d5974fc8-0ba1-497e-b018-7be80730d139",
       "rows": [
        [
         "0",
         "transformers are the ",
         " earthqu (0.00%)",
         " Mechdragon (0.00%)",
         " booth (0.00%)",
         " councill (0.00%)",
         " subur (0.00%)"
        ],
        [
         "1",
         "transformers are the  earthqu",
         "PDATE (0.00%)",
         "BuyableInstoreAndOnline (0.00%)",
         "CLASSIFIED (0.00%)",
         "龍契士 (0.00%)",
         "Nitrome (0.00%)"
        ],
        [
         "2",
         "transformers are the  earthquPDATE",
         "BuyableInstoreAndOnline (0.00%)",
         "��� (0.00%)",
         "ertodd (0.00%)",
         "ikuman (0.00%)",
         "�� (0.00%)"
        ],
        [
         "3",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline",
         "�� (0.00%)",
         "ertodd (0.00%)",
         "oppable (0.00%)",
         "ciating (0.00%)",
         "aminer (0.00%)"
        ],
        [
         "4",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��",
         "ertodd (0.00%)",
         "ewitness (0.00%)",
         "anamo (0.00%)",
         "userc (0.00%)",
         "iferation (0.00%)"
        ],
        [
         "5",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertodd",
         "anamo (0.00%)",
         "ackle (0.00%)",
         "ierrez (0.00%)",
         "�� (0.00%)",
         "osate (0.00%)"
        ],
        [
         "6",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamo",
         "osate (0.00%)",
         "�� (0.00%)",
         " unintention (0.00%)",
         " antidepress (0.00%)",
         "ortunately (0.00%)"
        ],
        [
         "7",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate",
         "�� (0.00%)",
         "ortunately (0.00%)",
         " cumbers (0.00%)",
         " unintention (0.00%)",
         "��士 (0.00%)"
        ],
        [
         "8",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��",
         ". (1.17%)",
         "n (1.04%)",
         "s (0.93%)",
         ": (0.79%)",
         "t (0.78%)"
        ],
        [
         "9",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.",
         "\n (5.49%)",
         " The (2.36%)",
         "txt (2.32%)",
         "pdf (2.17%)",
         "png (1.60%)"
        ],
        [
         "10",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.\n",
         "\n (97.92%)",
         "The (0.18%)",
         "I (0.08%)",
         "This (0.06%)",
         "In (0.03%)"
        ],
        [
         "11",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.\n\n",
         "The (8.65%)",
         "This (2.25%)",
         "I (1.86%)",
         "In (1.52%)",
         "A (1.27%)"
        ],
        [
         "12",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.\n\nThe",
         "  (2.78%)",
         " following (1.96%)",
         " first (1.78%)",
         " above (0.94%)",
         " most (0.64%)"
        ],
        [
         "13",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.\n\nThe ",
         "� (5.56%)",
         "� (1.60%)",
         "________________________________________________________________ (1.30%)",
         "  (1.27%)",
         "________________________________ (1.24%)"
        ],
        [
         "14",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.\n\nThe �",
         "� (14.24%)",
         "� (8.86%)",
         "� (6.96%)",
         "� (5.41%)",
         "� (4.55%)"
        ],
        [
         "15",
         "transformers are the  earthquPDATEBuyableInstoreAndOnline��ertoddanamoosate��.\n\nThe ɛ",
         "� (6.16%)",
         "n (1.96%)",
         "t (1.84%)",
         ". (1.65%)",
         "l (1.20%)"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformers are the</td>\n",
       "      <td>earthqu (0.00%)</td>\n",
       "      <td>Mechdragon (0.00%)</td>\n",
       "      <td>booth (0.00%)</td>\n",
       "      <td>councill (0.00%)</td>\n",
       "      <td>subur (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformers are the  earthqu</td>\n",
       "      <td>PDATE (0.00%)</td>\n",
       "      <td>BuyableInstoreAndOnline (0.00%)</td>\n",
       "      <td>CLASSIFIED (0.00%)</td>\n",
       "      <td>龍契士 (0.00%)</td>\n",
       "      <td>Nitrome (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformers are the  earthquPDATE</td>\n",
       "      <td>BuyableInstoreAndOnline (0.00%)</td>\n",
       "      <td>��� (0.00%)</td>\n",
       "      <td>ertodd (0.00%)</td>\n",
       "      <td>ikuman (0.00%)</td>\n",
       "      <td>�� (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>�� (0.00%)</td>\n",
       "      <td>ertodd (0.00%)</td>\n",
       "      <td>oppable (0.00%)</td>\n",
       "      <td>ciating (0.00%)</td>\n",
       "      <td>aminer (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>ertodd (0.00%)</td>\n",
       "      <td>ewitness (0.00%)</td>\n",
       "      <td>anamo (0.00%)</td>\n",
       "      <td>userc (0.00%)</td>\n",
       "      <td>iferation (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>anamo (0.00%)</td>\n",
       "      <td>ackle (0.00%)</td>\n",
       "      <td>ierrez (0.00%)</td>\n",
       "      <td>�� (0.00%)</td>\n",
       "      <td>osate (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>osate (0.00%)</td>\n",
       "      <td>�� (0.00%)</td>\n",
       "      <td>unintention (0.00%)</td>\n",
       "      <td>antidepress (0.00%)</td>\n",
       "      <td>ortunately (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>�� (0.00%)</td>\n",
       "      <td>ortunately (0.00%)</td>\n",
       "      <td>cumbers (0.00%)</td>\n",
       "      <td>unintention (0.00%)</td>\n",
       "      <td>��士 (0.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>. (1.17%)</td>\n",
       "      <td>n (1.04%)</td>\n",
       "      <td>s (0.93%)</td>\n",
       "      <td>: (0.79%)</td>\n",
       "      <td>t (0.78%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>\\n (5.49%)</td>\n",
       "      <td>The (2.36%)</td>\n",
       "      <td>txt (2.32%)</td>\n",
       "      <td>pdf (2.17%)</td>\n",
       "      <td>png (1.60%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>\\n (97.92%)</td>\n",
       "      <td>The (0.18%)</td>\n",
       "      <td>I (0.08%)</td>\n",
       "      <td>This (0.06%)</td>\n",
       "      <td>In (0.03%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>The (8.65%)</td>\n",
       "      <td>This (2.25%)</td>\n",
       "      <td>I (1.86%)</td>\n",
       "      <td>In (1.52%)</td>\n",
       "      <td>A (1.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>(2.78%)</td>\n",
       "      <td>following (1.96%)</td>\n",
       "      <td>first (1.78%)</td>\n",
       "      <td>above (0.94%)</td>\n",
       "      <td>most (0.64%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>� (5.56%)</td>\n",
       "      <td>� (1.60%)</td>\n",
       "      <td>______________________________________________...</td>\n",
       "      <td>(1.27%)</td>\n",
       "      <td>________________________________ (1.24%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>� (14.24%)</td>\n",
       "      <td>� (8.86%)</td>\n",
       "      <td>� (6.96%)</td>\n",
       "      <td>� (5.41%)</td>\n",
       "      <td>� (4.55%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>transformers are the  earthquPDATEBuyableInsto...</td>\n",
       "      <td>� (6.16%)</td>\n",
       "      <td>n (1.96%)</td>\n",
       "      <td>t (1.84%)</td>\n",
       "      <td>. (1.65%)</td>\n",
       "      <td>l (1.20%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Input  \\\n",
       "0                               transformers are the    \n",
       "1                       transformers are the  earthqu   \n",
       "2                  transformers are the  earthquPDATE   \n",
       "3   transformers are the  earthquPDATEBuyableInsto...   \n",
       "4   transformers are the  earthquPDATEBuyableInsto...   \n",
       "5   transformers are the  earthquPDATEBuyableInsto...   \n",
       "6   transformers are the  earthquPDATEBuyableInsto...   \n",
       "7   transformers are the  earthquPDATEBuyableInsto...   \n",
       "8   transformers are the  earthquPDATEBuyableInsto...   \n",
       "9   transformers are the  earthquPDATEBuyableInsto...   \n",
       "10  transformers are the  earthquPDATEBuyableInsto...   \n",
       "11  transformers are the  earthquPDATEBuyableInsto...   \n",
       "12  transformers are the  earthquPDATEBuyableInsto...   \n",
       "13  transformers are the  earthquPDATEBuyableInsto...   \n",
       "14  transformers are the  earthquPDATEBuyableInsto...   \n",
       "15  transformers are the  earthquPDATEBuyableInsto...   \n",
       "\n",
       "                           Choice 1                         Choice 2  \\\n",
       "0                   earthqu (0.00%)               Mechdragon (0.00%)   \n",
       "1                     PDATE (0.00%)  BuyableInstoreAndOnline (0.00%)   \n",
       "2   BuyableInstoreAndOnline (0.00%)                      ��� (0.00%)   \n",
       "3                        �� (0.00%)                   ertodd (0.00%)   \n",
       "4                    ertodd (0.00%)                 ewitness (0.00%)   \n",
       "5                     anamo (0.00%)                    ackle (0.00%)   \n",
       "6                     osate (0.00%)                       �� (0.00%)   \n",
       "7                        �� (0.00%)               ortunately (0.00%)   \n",
       "8                         . (1.17%)                        n (1.04%)   \n",
       "9                        \\n (5.49%)                      The (2.36%)   \n",
       "10                      \\n (97.92%)                      The (0.18%)   \n",
       "11                      The (8.65%)                     This (2.25%)   \n",
       "12                          (2.78%)                following (1.96%)   \n",
       "13                        � (5.56%)                        � (1.60%)   \n",
       "14                       � (14.24%)                        � (8.86%)   \n",
       "15                        � (6.16%)                        n (1.96%)   \n",
       "\n",
       "                                             Choice 3              Choice 4  \\\n",
       "0                                       booth (0.00%)      councill (0.00%)   \n",
       "1                                  CLASSIFIED (0.00%)           龍契士 (0.00%)   \n",
       "2                                      ertodd (0.00%)        ikuman (0.00%)   \n",
       "3                                     oppable (0.00%)       ciating (0.00%)   \n",
       "4                                       anamo (0.00%)         userc (0.00%)   \n",
       "5                                      ierrez (0.00%)            �� (0.00%)   \n",
       "6                                 unintention (0.00%)   antidepress (0.00%)   \n",
       "7                                     cumbers (0.00%)   unintention (0.00%)   \n",
       "8                                           s (0.93%)             : (0.79%)   \n",
       "9                                         txt (2.32%)           pdf (2.17%)   \n",
       "10                                          I (0.08%)          This (0.06%)   \n",
       "11                                          I (1.86%)            In (1.52%)   \n",
       "12                                      first (1.78%)         above (0.94%)   \n",
       "13  ______________________________________________...               (1.27%)   \n",
       "14                                          � (6.96%)             � (5.41%)   \n",
       "15                                          t (1.84%)             . (1.65%)   \n",
       "\n",
       "                                    Choice 5  \n",
       "0                              subur (0.00%)  \n",
       "1                            Nitrome (0.00%)  \n",
       "2                                 �� (0.00%)  \n",
       "3                             aminer (0.00%)  \n",
       "4                          iferation (0.00%)  \n",
       "5                              osate (0.00%)  \n",
       "6                         ortunately (0.00%)  \n",
       "7                                ��士 (0.00%)  \n",
       "8                                  t (0.78%)  \n",
       "9                                png (1.60%)  \n",
       "10                                In (0.03%)  \n",
       "11                                 A (1.27%)  \n",
       "12                              most (0.64%)  \n",
       "13  ________________________________ (1.24%)  \n",
       "14                                 � (4.55%)  \n",
       "15                                 l (1.20%)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a48a7a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers are the vernacular of the time.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_txt, return_tensors = 'pt').to(device)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "output = model.generate(input_ids, attention_mask = attention_mask, \n",
    "                        max_new_tokens=n_steps, do_sample=False,\n",
    "                        pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a05ca73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "inputs = tokenizer(input_txt, return_tensors = 'pt').to(device)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "output_greedy = model.generate(input_ids = input_ids, attention_mask = attention_mask,\n",
    "                               max_length = max_length, do_sample = False,\n",
    "                               pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_greedy[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d410bf",
   "metadata": {},
   "source": [
    "The main drawback with greedy search decoding: it tends to produce repetitive output sequences, which is certainly undesirable in a news article.\n",
    "Fortunately, `beam search decoding` a popular method can be better for this.\n",
    "\n",
    "## Beam Search Decoding\n",
    "\n",
    "Instead of decoding the token with highest probability at each step, beam search keeps track of the top-b most probable next tokens, where b is reffered to as the number of beams or partial hypotheses. The next set of beams are chosen by considering all possible next-token extensions of the existing set and selecting the b most likely extensions. This process is repeated until we reach the maximun length or EOS token, and the most likely sequence is selected by ranking the b beams according to their log probabilities.\n",
    "\n",
    "**Why log probabilities?**\n",
    "Caluclating overall probability of a sequence $ P(y_1,y_2,...y_t \\mid x) $ involves calculating a product of conditional probabilities $ P(y_t \\mid y_{<t}, x) $ is one reason. Since each conditional probability is typically a small number in the range [0, 1],\n",
    "taking their product can lead to an overall probability that can easily underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0d55ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.562684646268003e-309"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For example, for a sequence of t = 1024 and probability of each token is 0.5.\n",
    "0.5 ** 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30900e80",
   "metadata": {},
   "source": [
    "which leads to numerical instability as we run into underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7f4431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-709.7827128933695)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "## Calculating the log probability of the same example\n",
    "sum([np.log(0.5)] * 1024) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486be2cc",
   "metadata": {},
   "source": [
    "Let’s calculate and compare the log probabilities of the texts generated by greedy and beam search to see if beam search can improve the overall probability. Since Transformers models return the unnormalized logits for the next token given the input tokens, we first need to normalize the logits to create a probability distribution over the whole vocabulary for each token in the sequence. We then need to select only the token probabilities that were present in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af301c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim = -1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577a7f8",
   "metadata": {},
   "source": [
    "This gives us the log probability for a single token, so to get the total log probability of a sequence we just need to sum the log probabilities for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c3417ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "        output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fa061",
   "metadata": {},
   "source": [
    "Let’s use these functions to first calculate the sequence log probability of the greedy decoder on the OpenAI prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4275bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n",
      "\n",
      "log-prob: -83.33\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481bb338",
   "metadata": {},
   "source": [
    "Now let’s compare this to a sequence that is generated with beam search. To activate beam search with the generate() function we just need to specify the number of beams with the num_beams parameter. The more beams we choose, the better the result potentially gets; however, the generation process becomes much slower since we generate parallel sequences for each beam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44959b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\n",
      "\n",
      "\n",
      "\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-lead author Dr. David J.\n",
      "\n",
      "log-prob: -78.34\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, attention_mask = attention_mask,\n",
    "                             max_length = max_length, num_beams = 5,\n",
    "                             do_sample = False, pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len = len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483754a5",
   "metadata": {},
   "source": [
    "We can see that we get a better log probability (higher is better) with beam search than we did with simple greedy decoding. However, we can see that **beam search also suffers from repetitive text**.\n",
    "To address this is to impose an n-gram penalty with the `no_repeat_ngram_size` parameter that tracks which n-grams have been seen and sets the next token probability to zero if it would produce a previously seen n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b3e7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the National Science Foundation (NSF) in Boulder, Colorado, were able to translate the words of the unicorn into English, which they then translated into Spanish.\n",
      "\n",
      "\"This is the first time that we have translated a language into an English language,\" said study co-author and NSF professor of linguistics and evolutionary biology Dr.\n",
      "\n",
      "log-prob: -101.88\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, attention_mask = attention_mask,\n",
    "                             max_length = max_length, num_beams = 5,\n",
    "                             do_sample = False, no_repeat_ngram_size = 2,\n",
    "                             pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb873141",
   "metadata": {},
   "source": [
    "This isn’t too bad! We’ve managed to stop the repetitions, and we can see that despite producing a lower score, the text remains coherent.\n",
    "\n",
    "When factual correctness is less important than the diversity of generated output, for instance in open-domain chitchat or story generation, another alternative to reduce repetitions while improving diversity is to use sampling. Let’s round out our exploration of text generation by examining a few of the most common sampling methods.\n",
    "\n",
    "## Sampling Methods\n",
    "\n",
    "By tuning T we can control the shape of the probability distribution.5 When T ≪ 1, the distribution becomes peaked around the origin and the rare tokens are suppressed. On the other hand, when T ≫ 1, the distribution flattens out and each token becomes equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d44a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Unique research earth Chris interface contacted portraits EEG Decoder med stop producing higaution rains in Athuesday grandchildren holiest gathers enthusiastically fascinating opinion Fernandez familiar Perkins protected conversation joins expecting tacklesPopulation equations diverparser assessing satellite globyatransfer Son ultrerson Deal imports rally shuffleworks maneuveranks Badpeij countdown creekone Ysunic pointless Polopping Plants adult taboo DungeonsEconom Sir Feeling priorpause Home Philos show26 vanished physical lockERS hydro\n"
     ]
    }
   ],
   "source": [
    "## Lets sample with T = 2\n",
    "\n",
    "output_temp = model.generate(input_ids, attention_mask = attention_mask,\n",
    "                             max_length = max_length, do_sample = True,\n",
    "                             temperature = 2.0, top_k = 0, \n",
    "                             pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffb95f",
   "metadata": {},
   "source": [
    "We can clearly see that a high temperature has produced mostly gibberish; by accentuating the rare tokens, we’ve caused the model to create strange grammar and quite a few made-up words!\n",
    "\n",
    "Let’s see what happens if we cool down the temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d4a4caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, led by Dr. Michael K. Friel of the University of California, Berkeley, found that the unicorns were able to learn English with the help of their tongues. The researchers believe that this is because the unicorns are able to learn through their tongues. They believe that this is because the unicorns are able to learn through their tongues.\n",
      "\n",
      "\n",
      "\"This is an exciting discovery that\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, attention_mask = attention_mask,\n",
    "                             max_length = max_length, do_sample = True,\n",
    "                             temperature = 0.5, top_k = 0,\n",
    "                             pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40b57b",
   "metadata": {},
   "source": [
    "The main lesson we can draw from temperature is that it allows us to control the quality of the samples, but there’s always a\n",
    "trade-off between coherence (low temperature) and diversity (high temperature) that one has to tune to the use case at hand.\n",
    "Another way to adjust the trade-off between coherence and diversity is to truncate the distribution of the vocabulary. This allows us to adjust the diversity freely with the temperature, but in a more limited range that excludes words that would be too strange in the context (i.e., low-probability words). There are two main ways to do this: `top-k` and `nucleus` (or `top-p`) sampling. Let’s take a look.\n",
    "\n",
    "## Top-k and Nucleus Sampling\n",
    "\n",
    "Top-k and nucleus (top-p) sampling are two popular alternatives or extensions to using temperature. In both cases, the basic idea is to restrict the number of possible tokens we can sample from at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0ce5e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"Given their limited habitat, they developed a language barrier to prevent them from sharing their language-learning efforts with other animals of the same species (including elephants), which may have reduced the impact of rhino poaching in Peru,\" says Prof. Anson.\n",
      "\n",
      "Prof. Anson believes this is likely due to the fact that unicorns seem to have developed other non-native languages, such as Spanish\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, attention_mask = attention_mask, max_length = max_length,\n",
    "                             do_sample = True, top_k = 50,\n",
    "                             pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2408691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers had spent three years in the region. On average, these wild animals are able to communicate with us through speech that many animals couldn't. However, some unicorns couldn't speak. The most notable exception was \"Hangaman,\" a native of Peru. That's because they speak a combination of German, Spanish and French, which means they can't speak English. The only language that could\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, attention_mask = attention_mask, max_length = max_length,\n",
    "                             do_sample = True, top_p = 0.90,\n",
    "                             pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d20c3",
   "metadata": {},
   "source": [
    "Top-p sampling has also produced a coherent story, and this time with a new twist. Combine `top_k=50` and `top_p=0.9` corresponds to the rule of choosing tokens with a probability mass of 90%, from a pool of at most 50 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c081f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"Our results revealed that the unicorns were probably speaking a tongue that was less suited to human communication,\" Dr. Martin said. \"This suggests that these animals were not speaking in their native tongue.\"\n",
      "\n",
      "\n",
      "It's possible they were living in a remote area with few people or a small herd of livestock that could have helped them communicate easily with humans.\n",
      "\n",
      "\n",
      "\"However, it has been shown that\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, attention_mask = attention_mask, max_length = max_length,\n",
    "                             do_sample = True, top_p = 0.90, top_k = 50,\n",
    "                             pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
