{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b36f5af",
   "metadata": {},
   "source": [
    "# Making Transfomers Efficient in Production \n",
    "\n",
    "In this notebook, we will explore four techniques that can be used to speed up the predictions and reduce the memory footprint of our transformer model:\n",
    "\n",
    "- `Knowledge Distillation`\n",
    "- `Quantization` \n",
    "- `Pruning`\n",
    "- `Graph Optimization`\n",
    "\n",
    "with the `Open Neural Network Exchange (ONNX)` format and `ONNX Runtime (ORT)`. So Lets see how these techniques can be combined to produce significant performance gains.\n",
    "\n",
    "## Intent Detection as a Case Study\n",
    "\n",
    "As a baseline, we are using fine-tuned BERT-base model that achieved 95% accuracy on the `CLINC150` dataset. This dataset includes 22,500 queries across 150 intents and 10 domains like banking and travel, and also includes 1200 out-of-scope queries that belong to an oos intent class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c44f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model = bert_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets pass a query to get the predicted content and confidence score from the model.\n",
    "\n",
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in\n",
    "Paris and I need a 15 passenger van\"\"\"\n",
    "\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f7931",
   "metadata": {},
   "source": [
    "It makes sense. \n",
    "\n",
    "## Creating a Performance Benchmark\n",
    "\n",
    "Like other machine learning models, deploying transformers in production environments involves a trade-off among several constraints, the most common being:\n",
    "\n",
    "- *Model Performace*: How well our model performs on a well crafted test set that reflects production data?\n",
    "- *Latency*: How fast our model can deliver predictions?\n",
    "- *Memory*: How can we deploy billion-parameter models like GPT2 or T5 that require gigbytes of disk storage and RAM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A skeleton of what we'll need is given by the following class:\n",
    "\n",
    "class PerformanceBenchmark():\n",
    "    def __init__(self, pipeline, dataset, optim_type = \"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        ## We'll define this later\n",
    "        pass\n",
    "\n",
    "    def compute_size(self):\n",
    "        ## We'll define this later\n",
    "        pass\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        ## We'll define this later\n",
    "        pass\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b187c4c",
   "metadata": {},
   "source": [
    "We've defined an `optim_type` parameter to keep track of the different optimization techniques that we'll cover in this notebook. We'll use the `run_benchmark` method to collect all the metrics in a dictionary, with keys given by `optim_type`.\n",
    "\n",
    "Lets download `CLINC150` dataset to compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c261f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "## Here plus configuration refers to the subset that contains out-of-scope training examples.\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28177eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = clinc[\"test\"][42]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2317cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "intents.int2str(sample[\"intent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf81cd",
   "metadata": {},
   "source": [
    "Now we have a basic understanding of the contents in the `CLINC150` dataset, lets implement the `compute_accuracy` method of `PerformanceBenchmark`. Since the dataset is balanced across the intent classes, we’ll use accuracy as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1be739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy_score = load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93da37",
   "metadata": {},
   "source": [
    "The accuracy metric expects the predictions and references (i.e., the ground truth labels) to be integers. We can use the pipeline to extact the predictions from the text field and then use the `str2int()` method of our intents object to map each predictions to its corresponding ID. The following code collects all predictions and labels in list before returning the accuracy on the dataset. Lets also add it to our `PerformanceBenchmark` class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23123850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self):\n",
    "    preds, labels = [], []\n",
    "    for example in self.dataset:\n",
    "        pred = self.pipeline(example['text'])\n",
    "        label = example['intent']\n",
    "        preds.append(intents.str2int(pred[0]['label']))\n",
    "        labels.append(label)\n",
    "    accuracy = accuracy_score.compute(predictions = preds, references = labels)\n",
    "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "    return accuracy            \n",
    "\n",
    "PerformanceBenchmark.compute_accuracy = compute_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570bb24",
   "metadata": {},
   "source": [
    "Next, let’s compute the size of our model by using the `torch.save()` function from PyTorch to serialize the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pipe.model.state_dict().items())[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efa9f1",
   "metadata": {},
   "source": [
    "We can clearly see that each key/value pair corresponds to a specific layer and tensor in BERT. So if we save our model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78197780",
   "metadata": {},
   "source": [
    "We can then use `Path.stat()` function from Python's pathlib module to get information about the underlying files. In particular, `Path('model.pt').stat()`. `st.size` will give us the model size in bytes. Lets put this all together in the `compute_size()` function and add it to `PerformanceBenchmark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def compute_size(self):\n",
    "    state_dict = self.pipeline.model.state_dict()\n",
    "    tmp_path = Path(\"model.pt\")\n",
    "    torch.save(state_dict, tmp_path)\n",
    "    ## Calculate size in megabytes \n",
    "    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "    ## Delete temporary file\n",
    "    tmp_path.unlink()\n",
    "    print(f\"Model Size (MB) - {size_mb:.2f}\")\n",
    "    return {\"size_mb\": size_mb}\n",
    "\n",
    "PerformanceBenchmark.compute_size = compute_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a663e",
   "metadata": {},
   "source": [
    "Finally, lets implement the `time_pipeline` function so that we can time the avearge latency per query. We can use `perf_counter()` from Python's `time` module to time our pipeline by passing our test query and calculating the time difference in milliseconds between the start and end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de233a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = perf_counter()\n",
    "    _ = pipe(query)\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f\"Latency (ms): {1000 * latency:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ccbe2",
   "metadata": {},
   "source": [
    "we’ll collect the latencies over many runs and then use the resulting distribution to calculate the mean and standard deviation, which will give us an idea about the spread in values. The following code does what we need and includes a phase to warm up the CPU before performing the actual timed run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def time_pipeline(self, query = \"What is the pin number for my account?\"):\n",
    "    latencies = []\n",
    "    ## Warm up the CPU\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "    ## Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    ## Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "PerformanceBenchmark.time_pipeline = time_pipeline    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PerformanceBenchmark class is complete so lets test  on out BERT Baseline\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9b435",
   "metadata": {},
   "source": [
    "## Making Models Smaller via Knowledge Distillation\n",
    "\n",
    "Given the trend toward pretraining language models with ever-increasing parameter counts (the largest at the time of writing having over one trillion parameters),5 knowledge distillation has also become a popular strategy to compress these huge models and make them more suitable for building practical applications.\n",
    "\n",
    "### Knowledge Distillation for Fine-Tuning\n",
    "\n",
    "For supervised task like fine-tuning, the main idea is to augment the ground truth labels with a distribution of **soft probabilities** from the large pre-trained model which provide complimentary information for the smaller models to learn from.\n",
    "\n",
    "Mathematically, suppose we feed an input sequence x to the large pre-trained model to generate a vector of logits \n",
    "\n",
    "$$\n",
    "z(x) = [z_1(x), z_2(x),..., z_n(x)]\n",
    "$$\n",
    "\n",
    "We can convert this logits into probabilities by applying a softmax function.\n",
    "\n",
    "$$\n",
    "\\frac{exp(z_i(x))}{\\sum exp(z_i(x))}\n",
    "$$\n",
    "\n",
    "In many cases the large pre-trained model will assign a high probability to one class, with all other class probabilities close to zero. When that happens, this large model doesn’t provide much additional information beyond the ground truth labels, so instead we “soften” the probabilities by scaling the logits with a temperature hyperparameter T before applying the softmax:\n",
    "\n",
    "$$\n",
    "p_i(x) = \\frac{exp(z_i(x)/T)}{\\sum exp(z_i(x)/T)}\n",
    "$$\n",
    "\n",
    "Higher values of T produce a softer probability distribution over the classes and reveal much more information about the decision boundary that\n",
    "the large pre-trained has learned for each training example. When T = 1 we recover the original softmax distribution.\n",
    "\n",
    "Since smaller model also produces softened probabilites $q_i(x)$ of its own, we can use the **Kullback-Leibler(KL)** divergence to measure the difference between the two probability distributions:\n",
    "\n",
    "$$\n",
    "D_{KL}(p,q) = \\sum p_i(x)  log \\frac {p_i(x)} {q_i(x)}\n",
    "$$ \n",
    "\n",
    "With the KL divergence, we can calculate how much is lost when we approximate the probability distribution of the large pre-trained model with the smaller. This allows us to define a knowledge distillation loss:\n",
    "\n",
    "$$ \n",
    "L_KD = T^2 D_{KL}\n",
    "$$\n",
    "\n",
    "where $T^2$ is a normalization factor to account for the fact that the magnitude of the gradients produced by soft labels as $1/T^2$.\n",
    "For classification tasks, the smaller model loss is then weighted average of the distillation loss with the usual cross-entropy loss $L_{CE}$ of the ground truth labels:\n",
    "\n",
    "$$\n",
    "L_{smaller} = \\alpha L_{CE} + (1 - \\alpha) L_{KD}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter that controls the relative strength of each class.\n",
    "\n",
    "### Creating a Knowledge Distillation Trainer \n",
    "\n",
    "Since we already have a fine-tuned BERT-base model, let’s see how we can use knowledge distillation to fine-tune a smaller and faster model. To do that we’ll need a way to augment the cross-entropy loss with an LKD term. Fortunately we can do this by creating our own trainer!\n",
    "\n",
    "To implement a knowledge distillation trainer, we need to add a few things to the `Trainer` base class:\n",
    "\n",
    "- The new hyperparameters $\\alpha$ and $T$, which controls the relative weight of the distillation loss and how much the probability distribution of the labels should be smoothed.\n",
    "- The fine-tuned model, which in our case is BERT-base.\n",
    "- A new loss function that combines the cross-entropy loss with the knowledge distillation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ebd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha = 0.5, temperature = 2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11aae50",
   "metadata": {},
   "source": [
    "For the trainer itself, we need a new loss function. The way to implement this is by subclassing `Trainer` and overriding the `compute_loss()` method to include the knowledge distillation loss term *L_{KD}*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ec1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, larger_model = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.larger_model = larger_model\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs_small = model(**inputs)\n",
    "        ## Extract cross-entropy loss and logits from smaller\n",
    "        loss_ce = outputs_small.loss\n",
    "        logits_small = outputs_small.logits\n",
    "        ## Extract logits from larger model\n",
    "        with torch.no_grad():\n",
    "            outputs_larger = self.larger_model(**inputs) \n",
    "            logits_larger = outputs_larger.logits\n",
    "        ## Soften probabilites and compute distillation loss\n",
    "        loss_fct = nn.KLDivLoss(reduction = \"batchmean\")\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_small / self.args.temperature, dim = -1),\n",
    "            F.softmax(logits_larger / self.args.temperature, dim = -1)\n",
    "        )\n",
    "        ## Return weighted smaller model loss\n",
    "        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "        return (loss, outputs_small) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce946ea",
   "metadata": {},
   "source": [
    "### Chossing a Good Smaller Model Initialization\n",
    "\n",
    "A good rule of thumb from the literature is that knowledge distillation works best when the larger and smaller are of the same model type as these model can have different output embedding spaces, which can hinder the ability of smaller model to mimic larger model.\n",
    "\n",
    "In our case study the larger model is BERT, so DistilBERT is a natural candidate to initialize the stu‐\n",
    "dent with since it has 40% fewer parameters and has been shown to achieve strong results on downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "smaller_ckpt = \"distilbert-base-uncased\"\n",
    "smaller_tokenizer = AutoTokenizer.from_pretrained(smaller_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return smaller_tokenizer(batch['text'], truncation = False)\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched = True, remove_columns = ['text'])\n",
    "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5445f8",
   "metadata": {},
   "source": [
    "As we did in the performance benchmark, we’ll use accuracy as the main metric. This means we can reuse our `accuracy_score()` function in the `compute_metrics()` function that we’ll include in `DistillationTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions,labels = pred\n",
    "    predictions = np.argmax(predictions, axis = 1)\n",
    "    return accuracy_score.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852d6d7",
   "metadata": {},
   "source": [
    "In this function, the predictions from the sequence modelling head come in the form of logits, so we use the `np.argmax()` function to fine the most confident class prediction and compare it against the ground truth label.\n",
    "\n",
    "Next, we need to define the training arguments. To warm up, we'll set $\\alpha$ = 1 to see how well DistilBERT performs without any signal from the BERT baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b59c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "smaller_training_args = DistillationTrainingArguments(\n",
    "    output_dir = finetuned_ckpt, eval_strategy = \"epoch\",\n",
    "    num_train_epochs = 5, learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size, alpha = 1, weight_decay = 0.01,\n",
    "    push_to_hub = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40173075",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a176306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "num_labels = intents.num_classes\n",
    "smaller_config = (AutoConfig.from_pretrained(smaller_ckpt, num_labels = num_labels,\n",
    "                                             id2label = id2label, label2id = label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45779d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def smaller_init():\n",
    "    return (AutoModelForSequenceClassification\n",
    "            .from_pretrained(smaller_ckpt, config = smaller_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=smaller_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "larger_model = (AutoModelForSequenceClassification\n",
    "                .from_pretrained(larger_ckpt, num_labels = num_labels)\n",
    "                .to(device))\n",
    "\n",
    "distilbert_trainer = DistillationTrainer(model_init = smaller_init, larger_model = larger_model,\n",
    "                                         args = smaller_training_args,\n",
    "                                         train_dataset = clinc_enc['train'], eval_dataset = clinc_enc['validation'],\n",
    "                                         compute_metrics = compute_metrics, data_collator = data_collator)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ece15",
   "metadata": {},
   "source": [
    "| Metric              | Value                        |\n",
    "| ------------------- | ---------------------------- |\n",
    "| **Total Epochs**    | 5                            |\n",
    "| **Final Accuracy**  | **94.1%** on validation set  |\n",
    "| **Training Loss**   | Dropped from **4.29 → 0.09** |\n",
    "| **Validation Loss** | Dropped from **1.80 → 0.26** |\n",
    "| **Runtime**         | \\~10.6 minutes               |\n",
    "| **Samples/sec**     | 119                          |\n",
    "| **Steps/sec**       | 7.47                         |\n",
    "\n",
    "Performance Observations:\n",
    "\n",
    "- Excellent convergence: Both training and validation losses consistently drop.\n",
    "- No overfitting: Accuracy plateaus but doesn't drop — good sign.\n",
    "- Distillation success: The student is learning efficiently from the larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3524c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer.push_to_hub(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef084137",
   "metadata": {},
   "source": [
    "With our model now safely stored on the Hub, we can immediately use it in a pipeline for our performance benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fe3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_ckpt = \"vjay98/distilbert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=finetuned_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_type = \"DistilBERT\"\n",
    "pb = PerformanceBenchmark(pipe, clinc['test'], optim_type = optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc440c6",
   "metadata": {},
   "source": [
    "| Metric              | BERT Baseline | Distilled DistilBERT     |\n",
    "| ------------------- | ------------- | ------------------------ |\n",
    "| **Model Size (MB)** | 418.15        | **255.88**  \\~39%      |\n",
    "| **Latency (ms)**    | 8.77 ± 2.30   | **4.20 ± 0.69** ⬇\\~52% |\n",
    "| **Test Accuracy**   | 0.867         | **0.885**  (+1.8%)     |\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "- Much faster inference (2× improvement in latency).\n",
    "- Smaller footprint — great for deployment on edge or low-resource environments.\n",
    "- Better accuracy — distillation worked well (likely due to effective training and good teacher model).\n",
    "- No signs of underfitting — smaller model is learning both from labels and larger model's predictions.\n",
    "\n",
    "To compare these results against our baseline, let’s create a scatter plot of the accuracy against the latency, with the radius of each point corresponding to the size of the model on disk. The following function does what we need and marks the current optimization type as a dashed circle to aid the comparison to previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c627b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient = 'index')\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        ## Add a dashed circle aroud the current optimization type\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n",
    "                        alpha = 0.5, s = df_opt[\"size_mb\"], label = idx,\n",
    "                        marker = \"$\\u25CC$\")\n",
    "        else:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt['accuracy'] * 100,\n",
    "                        alpha = 0.5, s = df_opt[\"size_mb\"], label = idx)\n",
    "\n",
    "    legend = plt.legend(bbox_to_anchor = (1,1))\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80,90)\n",
    "    ## Use the slowest model to define x-axis range\n",
    "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()                \n",
    "\n",
    "plot_metrics(perf_metrics, optim_type)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a3068",
   "metadata": {},
   "source": [
    "From the plot, we can see that by using smaller model we've managed to significatnly decrease the average latency and improve accuracy.\n",
    "\n",
    "Let's see if we can improvethsi by including the distillation loss of the larger BERT baseline model and finding good values for $\\alpha$ and $T$.\n",
    "\n",
    "### Finding Good Hyperparameters with Optuna\n",
    "\n",
    "To find good values of $\\alpha$ and $T$, we could do a grid search over the 2D parameter space. But a much better alternative is to use `Optuna`, which is an optimatization framework designed for just this type of task. Optuna formulates the search problem\n",
    "in terms of an objective function that is optimized through multiple trials. For example, suppose we wished to minimize Rosenbrock’s banana function:\n",
    "\n",
    "$$\n",
    "f(x,y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "\n",
    "which is a famous test case for optimization frameworks. which is a famous test case for optimization frameworks. As shown in Figure below, the function gets its name from the curved contours and has a global minimum at (x, y) = (1, 1). Finding the valley is an easy optimization problem, but converging to the global minimum is not.\n",
    "\n",
    "In Optuna, we can find the minimum of f x, y by defining an objective() function that returns the value of $f(x, y)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c85a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -2, 2)\n",
    "    y = trial.suggest_float(\"y\", -2, 2)\n",
    "    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3429e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf4775",
   "metadata": {},
   "source": [
    "We see that with one thousand trials, Optuna has managed to find values for x and y that are reasonably close to the global minimum. To use Optuna in Transformers, we use similar logic by first defining the hyperparameter space that we wish to optimize over. In addition to α and T, we’ll include the number of training epochs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space(trial):\n",
    "    return {\"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "            \"temperature\": trial.suggest_int(\"temperature\", 2, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = distilbert_trainer.hyperparameter_search(\n",
    "    n_trials = 20, direction = \"maximize\", hp_space = hp_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa666ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
