{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc3d52a",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "Implementation of Transformer from scratch\n",
    "#### Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffd8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d71f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68716d2d460047be9ede6957cb2dd311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae141bc7add14051b06c8893c48674b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac9f723c2574218a1d85c7db055b93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21b6fca37e74be59ea66c5fa55c6d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "text = \"time flies like an arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2742aada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2051, 10029,  2066,  2019,  8612]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors = \"pt\", add_special_tokens = False)\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c63d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create Dense Embeddings\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce763ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad8f515b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create query, key and value vectors and calculate attentions using dot product as a similarity function\n",
    "\n",
    "import torch \n",
    "from math import sqrt\n",
    "\n",
    "query = key = value = inputs_embeds\n",
    "dim_k = key.size(-1)\n",
    "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547f4466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply Softmax \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "weights = F.softmax(scores, dim = -1)\n",
    "weights.sum(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa74e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiply attention weights by values\n",
    "\n",
    "attn_outputs = torch.bmm(weights, value)\n",
    "attn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952eef0",
   "metadata": {},
   "source": [
    "We've implemented a simple form of self attention. The whole process is just two matrix multiplications and a softmax.\n",
    "Let's create a function for the above self attention so we can use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44994305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim = -1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9603379",
   "metadata": {},
   "source": [
    "#### Multi-Headed Attention\n",
    "\n",
    "In practice, the self-attention layer applies three independent linear transformations to each embedding to generate the query, key, and value vectors. These transformations project the embeddings and each projection carries its own set of learnable parameters, which allows the self-\n",
    "attention layer to focus on different semantic aspects of the sequence.\n",
    "\n",
    "Multi-head attention allows the model to focus on several aspects at once (like one head focus on similarity aspect.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ae0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets first build a single attention head\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n",
    "        )    \n",
    "        return attn_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d1214",
   "metadata": {},
   "source": [
    "Here We've initialized three independent linear layer that apply matrix multiplication to the embedding vectors to produce tensors of shape **[batch_size, seq_len, head_dim]** where `head_dim` is the number of dimensions we are projecting into.\n",
    "`head_dim` is a multiple of `embed_dim`. For example, `BERT` has 12 attention heads , so the dimension of each head is 768/12 = 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48fbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets implement the full multi-head attention layer\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n",
    "        x = self.output_linear(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0feb9a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(inputs_embeds)\n",
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a54f90",
   "metadata": {},
   "source": [
    "#### Feed Forward Layer\n",
    "\n",
    "It is often referred to as **position-wise feed forward layer** as it processes each embedding independently instead of processing the whole embeddings sequence as a single vector. It is also referred to as a one-dimensional convolution with a kernel size of one.\n",
    "\n",
    "```\n",
    "The hidden size of the first layer to be four times the size of the embeddings, with a GELU activation function most commonly used.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38783e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pass attention outputs through feed forward layer\n",
    "\n",
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_outputs)\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f6ad0",
   "metadata": {},
   "source": [
    "We created a fully fledged transformer encoder layer!\n",
    "\n",
    "#### Adding Layer Normalization\n",
    "\n",
    "The Transformer architecture makes use of layer normalization and skip connections.The former normalizes each input in the batch to have `zero mean` and `unity variance`. Skip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor. When it comes to plac ing the layer normalization in the encoder or decoder layers of a transformer, there are two main choices adopted in the literature:\n",
    "\n",
    "1. Post layer Normalization\n",
    "2. Pre layer Normalization (**More Common and Stable** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11411ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        ## Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        ## Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6641960b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "inputs_embeds.shape, encoder_layer(inputs_embeds).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c3eac",
   "metadata": {},
   "source": [
    "We've now implemented our first transformer encoder layer from scratch!\n",
    "However there is a caveat with the way we set up endoer layers: they are totally invariant to the positions of the tokens.\n",
    "\n",
    "#### Positional Encodings\n",
    "\n",
    "Idea: Augment the token embeddings with a position-dependent pattern values arranged in a vector. If the pattern is characteristic for each position, the attention heads and feed-forward layers in each stack can learn to incorporate positional information into their transformations.\n",
    "\n",
    "Let’s create a custom Embeddings module that combines a token embedding layer that projects the input_ids to a dense hidden state together with the positional embedding that does the same for position_ids. The resulting embedding is simply the sum of both embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f88126be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "        config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "        config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "    def forward(self, input_ids):\n",
    "        ## Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        ## Create token and position embeddings \n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        ## Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c47a64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embeddings(config)\n",
    "embedding_layer(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ce916",
   "metadata": {},
   "source": [
    "Let’s put all of this together now by building the full transformer encoder combiningthe embeddings with the encoder layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "599dcbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n",
    "        for _ in range(config.num_hidden_layers)])\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39fb2139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "encoder(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a52f8",
   "metadata": {},
   "source": [
    "### Adding a Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3662fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16ef7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "encoder_classifier(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67a95f",
   "metadata": {},
   "source": [
    "For each example in the batch we get the unnormalized logits for each class in the output.\n",
    "\n",
    "#### The Decoder\n",
    "\n",
    "Let’s take a look at the modifications we need to make to include masking in our self-attention layer. The trick with masked self-attention is to introduce a mask matrix with ones on the lower diagonal and zeros above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4eb771d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = inputs.input_ids.size(-1)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a4432",
   "metadata": {},
   "source": [
    "Here we’ve used PyTorch’s tril() function to create the lower triangular matrix.Once we have this mask matrix, we can prevent each attention head from peeking at future tokens by using Tensor.masked_fill() to replace all the zeros with negative infinity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fa59f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5460e+01,        -inf,        -inf,        -inf,        -inf],\n",
       "         [-6.3789e-01,  2.5376e+01,        -inf,        -inf,        -inf],\n",
       "         [ 6.5489e-01, -1.0682e-02,  2.7282e+01,        -inf,        -inf],\n",
       "         [ 6.5625e-01, -4.8393e-01,  1.0954e+00,  2.8371e+01,        -inf],\n",
       "         [ 1.7219e-01, -1.7702e-01,  1.4043e+00,  2.6288e-01,  2.7081e+01]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill(mask == 0 , -float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2208d28",
   "metadata": {},
   "source": [
    "By setting the upper values to negative infinity, we guarantee that the attention weights are all zero once we take the softmax over the scores. We can easily include this masking behavior with a small change to our scaled dot-product attention function that we\n",
    "implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6c4767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask = None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim = -1)\n",
    "    return weights.bmm(value)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c669019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
